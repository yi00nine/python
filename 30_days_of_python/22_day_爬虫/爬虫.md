# DAY22

## 爬虫


互联网上有大量的数据，这些数据可用于不同的目的。要收集这些数据，我们需要知道如何从网站上抓取数据。

网络抓取是从网站中提取和收集数据，并将其存储到本地机器或数据库中的过程。

在本节中，我们将使用 beautifulsoup 和 requests 软件包来抓取数据。我们使用的软件包版本是 beautifulsoup 4。

要开始刮擦网站，你需要 requests、beautifoulSoup4 和一个网站。

```
pip install requests
pip install beautifulsoup4
```

要从网站上抓取数据，需要对 HTML 标记和 CSS 选择器有基本的了解。我们使用 HTML 标签、类或/和 id 来定位网站内容。让我们导入请求和 BeautifulSoup 模块

```
import requests
from bs4 import BeautifulSoup
```

让我们为要搜索的网站声明 url 变量。

```py

import requests
from bs4 import BeautifulSoup
url = 'https://archive.ics.uci.edu/ml/datasets.php'

# Lets use the requests get method to fetch the data from url

response = requests.get(url)
# lets check the status
status = response.status_code
print(status) # 200 means the fetching was successful
```

使用 beautifulSoup 解析页面内容

```py
import requests
from bs4 import BeautifulSoup
url = 'https://archive.ics.uci.edu/ml/datasets.php'

response = requests.get(url)
content = response.content # 拿到网站内容
soup = BeautifulSoup(content, 'html.parser') # beautiful soup 解析
print(soup.title) 
print(soup.title.get_text()) # UCI Machine Learning Repository: Data Sets
print(soup.body) 
print(response.status_code)

tables = soup.find_all('table', {'cellpadding':'3'})
# 我们的目标表格的 cellpadding 属性值为 3
# 我们可以使用 id、class 或 HTML 标签进行选
table = tables[0] # the result is a list, we are taking out data from it
for td in table.find('tr').find_all('td'):
    print(td.text)
```
